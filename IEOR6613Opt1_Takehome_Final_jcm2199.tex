\documentclass{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb, amsmath}

\begin{document}

\title{IEOR 6613 - Optimization I: Take-home Final Examination}

\author{John Min\\ jcm2199}
\date{December 20, 2013}
\maketitle

\section{Nonstandard big-M simplex}
The nonstandard big-M method without adding artificial variables is quite similar to the simplex method.  The high-level intuition behind it is to be able to begin with an infeasible basic solution and take steps toward achieving a basic feasible solution.  The variables that would be entering the basis would be those that reduce cost, and the fastest way to reduce cost is to make a basic variable non-negative.  Once a BFS is found, then, the standard simplex method is applied to solve for an optimal solution.  If a BFS cannot be found thru this nonstandard big-M method, then, the primal of this problem is infeasible. \\

\noindent
\textbf{(a)}

\section{Logarithmic barrier}

\textbf(a)
Let $f(x) = c^\top x, g(x) = - \mu \sum_j ln(x_j)$.  $c^\top $ is both convex and concave as it is linear.   As a result of the Mean Value Theorem, a function $g$ is strictly convex on an open interval $(a,b) \Leftrightarrow$ its derivative $g'$ is strictly increasing, and thus, $g$ is a strictly convex function: \\

\noindent
Clearly, $g'(x) = -\mu \sum_j  \frac{1}{x_j}$ is strictly increasing.\\

\noindent
Let $x,y,z > 0$ where $x < y < z$.  By the Mean Value Theorem, \\
$\exists a : \frac{f(y) - f(x)}{y - x} = f'(a)$ where $x < a < y$. \\
$ \exists b: \frac{f(z) - f(y)}{z - y} = f'(b)$ where $y < b < z$. \\

\noindent
Since $f'(a) < f'(b), \frac{f(y) - f(x)}{y - x} < \frac{f(z) - f(y)}{z - y}$. \\

\noindent
In the case of linearity, .
$g$ is defined as above:  let $g(x) = - \mu \sum_j ln(x_j)$.  Define $h(x) = f(x) + g(x)$, which is the objective function of the primal of the logarithmic barrier problem ($P_\mu$).  \\

\noindent
For $\lambda \in (0,1)$, as $f$ is linear, $f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$.  As $g$ is strictly convex, for $y > x, g(\lambda x + (1-\lambda) y) = \lambda g(x) + (1-\lambda) g(y)$.  Hence, $h(\lambda x + (1-\lambda) y) = \lambda h(x) + (1-\lambda) h(y)$. \\

\noindent 
The proof to demonstrate the strict concavity of the logarithmic barrier problem dual's objective function $(D_\mu)$ is analogous to the above.  If a function $f$ is strictly convex, then $-f$ is strictly concave.  This can easily be shown analogous to the proof that employs the Mean Value Theorem as above.  The objective function of the $(D_\mu)$ contains a linear term and then, a $+ \mu \sum_j ln(x_j)$ term, whose first derivative is strictly decreasing.  As above, the summation of a linear function with a strictly concave function is a strictly concave function. \\

\noindent
\textbf{(b)}
$(iii) \Rightarrow (i)$:  Lemma 9.5\cite{BT} demonstrates that if the KKT conditions are met, $x^*$ is the unique optimal solution to the primal barrier problem.  

$(iii) \Rightarrow (ii)$:  The proof of $(iii) \Rightarrow (i)$ demonstrates that  given the satisfaction of the KKT conditions, $x^*$ is not only the unique optimal solution ot the primal, but also $x^*_j = \mu/s_j^*$.  Note the following:  $\mathbf{s,x > 0}$, $\mathbf{x^*}$ is unique, $s_j^* = x_j^*/\mu$.  Since $\mathbf{s^*} = \frac{1}{\mu} \mathbf{x^*}$ and $\mathbf{x^*}$ is unique, it is clear that the optimal solution of $D_\mu$ is unique in terms of $\mathbf{s}$.  

Under strong duality, $x^*$ and $y^*, s^*$  are optimal solutions to the primal and dual, respectively $\Leftrightarrow x^*$ and $y^*, s^*$ satisfy the KKT conditions. \\
Therefore, all that is left is to show that the existence of a unique optimal solution, $x^*$ for $P_\mu \Leftrightarrow$ unique optimal solution for $D_\mu$ in terms of $s$.  \\



\textbf{(e)}
\begin{thebibliography}{9}

\bibitem{BT}
Bertsimas, Dimitris \& Tsitsiklis, John N.  Introduction to Linear Optimization.  Athena Scientific \& Dynamic Ideas, LLC, Belmont, MA, 1997. 

\end{thebibliography}

\end{document}